{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK/5XxCTTKf1UGHNuVIOWb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t-snd/cartPole/blob/main/runCartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QuUnA8Fjs1VG",
        "outputId": "fd40bd25-1371-427f-f95c-4bb45e683fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training REINFORCE\n",
            "REINFORCE Episode 0, Avg Reward: 14.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:519: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REINFORCE Episode 100, Avg Reward: 19.02\n",
            "REINFORCE Episode 200, Avg Reward: 23.30\n",
            "REINFORCE Episode 300, Avg Reward: 27.34\n",
            "REINFORCE Episode 400, Avg Reward: 33.68\n",
            "REINFORCE Episode 500, Avg Reward: 41.95\n",
            "REINFORCE Episode 600, Avg Reward: 41.45\n",
            "REINFORCE Episode 700, Avg Reward: 49.55\n",
            "REINFORCE Episode 800, Avg Reward: 53.29\n",
            "REINFORCE Episode 900, Avg Reward: 65.83\n",
            "REINFORCE Episode 1000, Avg Reward: 69.97\n",
            "REINFORCE Episode 1100, Avg Reward: 82.05\n",
            "REINFORCE Episode 1200, Avg Reward: 130.43\n",
            "REINFORCE Episode 1300, Avg Reward: 155.99\n",
            "REINFORCE Episode 1400, Avg Reward: 137.05\n",
            "REINFORCE Episode 1500, Avg Reward: 153.22\n",
            "REINFORCE Episode 1600, Avg Reward: 175.13\n",
            "REINFORCE Episode 1700, Avg Reward: 185.55\n",
            "REINFORCE Episode 1800, Avg Reward: 191.20\n",
            "REINFORCE Episode 1900, Avg Reward: 153.98\n",
            "REINFORCE Episode 2000, Avg Reward: 195.40\n",
            "REINFORCE Solved in 2000 episodes!\n",
            "\n",
            "Training ACTOR-CRITIC\n",
            "ACTOR-CRITIC Episode 0, Avg Reward: 19.00\n",
            "ACTOR-CRITIC Episode 100, Avg Reward: 23.15\n",
            "ACTOR-CRITIC Episode 200, Avg Reward: 31.25\n",
            "ACTOR-CRITIC Episode 300, Avg Reward: 43.95\n",
            "ACTOR-CRITIC Episode 400, Avg Reward: 54.76\n",
            "ACTOR-CRITIC Episode 500, Avg Reward: 58.82\n",
            "ACTOR-CRITIC Episode 600, Avg Reward: 87.94\n",
            "ACTOR-CRITIC Episode 700, Avg Reward: 106.72\n",
            "ACTOR-CRITIC Episode 800, Avg Reward: 143.46\n",
            "ACTOR-CRITIC Episode 900, Avg Reward: 153.35\n",
            "ACTOR-CRITIC Episode 1000, Avg Reward: 162.96\n",
            "ACTOR-CRITIC Episode 1100, Avg Reward: 175.74\n",
            "ACTOR-CRITIC Episode 1200, Avg Reward: 184.81\n",
            "ACTOR-CRITIC Episode 1300, Avg Reward: 176.36\n",
            "ACTOR-CRITIC Episode 1400, Avg Reward: 180.08\n",
            "ACTOR-CRITIC Episode 1500, Avg Reward: 180.23\n",
            "ACTOR-CRITIC Episode 1600, Avg Reward: 178.50\n",
            "ACTOR-CRITIC Episode 1700, Avg Reward: 194.19\n",
            "ACTOR-CRITIC Episode 1800, Avg Reward: 193.54\n",
            "ACTOR-CRITIC Solved in 1854 episodes!\n",
            "\n",
            "Training PPO\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'unsqueez'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b6d628551062>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTraining {algo.upper()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b6d628551062>\u001b[0m in \u001b[0;36mtrain_algorithm\u001b[0;34m(algorithm, env_name, num_episodes, lr)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mactor_critic_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ppo'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mppo_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grpo'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mtrajectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b6d628551062>\u001b[0m in \u001b[0;36mppo_update\u001b[0;34m(net, optimizer, states, actions, total_reward, clip_ratio, value_loss_coef, entropy_coef)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mratios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_action_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m              \u001b[0mold_action_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0msurr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratios\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0msurr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mclip_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclip_ratio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'unsqueez'"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# 共通のポリシーネットワーク（REINFORCEとGRPO用）\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 共通のActor-Critic/PPOネットワーク\n",
        "class ActorCriticNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorCriticNet, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.actor(state), self.critic(state)\n",
        "\n",
        "# 共通のエピソード実行関数\n",
        "def run_episode(env, agent, algorithm, max_steps=200):\n",
        "    state, _ = env.reset()\n",
        "    states, actions, log_probs = [], [], []\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(max_steps):\n",
        "        state_tensor = torch.FloatTensor(state)\n",
        "        if algorithm in ['reinforce', 'grpo']:\n",
        "            logits = agent(state_tensor)\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            action = torch.multinomial(probs, 1).item()\n",
        "            log_prob = torch.log(probs[action])\n",
        "        else:  # actor-critic or ppo\n",
        "            action_logits, _ = agent(state_tensor)\n",
        "            probs = torch.softmax(action_logits, dim=-1)\n",
        "            action = torch.multinomial(probs, 1).item()\n",
        "            log_prob = torch.log(probs[action])\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        log_probs.append(log_prob)\n",
        "\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, log_probs, episode_reward\n",
        "\n",
        "# REINFORCEの実装\n",
        "def reinforce_update(net, optimizer, states, actions, log_probs, total_reward):\n",
        "    normalized_reward = total_reward / 200.0\n",
        "    policy_loss = []\n",
        "    for log_prob in log_probs:\n",
        "        policy_loss.append(-log_prob * normalized_reward)\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    return policy_loss.item()\n",
        "\n",
        "# Actor-Criticの実装\n",
        "def actor_critic_update(net, optimizer, states, actions, total_reward, gamma=0.99):\n",
        "    states = torch.FloatTensor(states)\n",
        "    actions = torch.LongTensor(actions)\n",
        "\n",
        "    actor_losses, critic_losses = [], []\n",
        "    for t in range(len(states) - 1):\n",
        "        action_logits, value = net(states[t].unsqueeze(0))\n",
        "        _, next_value = net(states[t+1].unsqueeze(0))\n",
        "        advantage = total_reward - value.detach()\n",
        "\n",
        "        action_probs = torch.softmax(action_logits, dim=-1)\n",
        "        log_prob = torch.log(action_probs[0, actions[t]])\n",
        "        actor_losses.append(-log_prob * advantage)\n",
        "        critic_losses.append(advantage.pow(2))\n",
        "\n",
        "    total_loss = torch.stack(actor_losses).sum() + 0.5 * torch.stack(critic_losses).sum()\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    return total_loss.item()\n",
        "\n",
        "# PPOの実装\n",
        "def ppo_update(net, optimizer, states, actions, total_reward, clip_ratio=0.2, value_loss_coef=0.5, entropy_coef=0.01):\n",
        "    states = torch.FloatTensor(states)\n",
        "    actions = torch.LongTensor(actions)\n",
        "\n",
        "    old_action_probs, old_values = net(states)\n",
        "    old_action_probs = old_action_probs.detach()\n",
        "    old_values = old_values.detach()\n",
        "\n",
        "    curr_action_probs, curr_values = net(states)\n",
        "    advantages = total_reward - old_values.squeeze()\n",
        "\n",
        "    ratios = curr_action_probs.gather(1, actions.unsqueeze(1)).squeeze() / \\\n",
        "             old_action_probs.gather(1, actions.unsqueez(1)).squeeze()\n",
        "    surr1 = ratios * advantages\n",
        "    surr2 = torch.clamp(ratios, 1 - clip_ratio, 1 + clip_ratio) * advantages\n",
        "    policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "    value_loss = (curr_values.squeeze() - total_reward).pow(2).mean()\n",
        "    entropy_loss = -(curr_action_probs * torch.log(curr_action_probs + 1e-10)).sum(dim=1).mean()\n",
        "\n",
        "    total_loss = policy_loss + value_loss_coef * value_loss - entropy_coef * entropy_loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    return total_loss.item()\n",
        "\n",
        "# GRPOの実装\n",
        "def grpo_update(net, optimizer, trajectories, eps=0.2, n_iterations=10):\n",
        "    rewards = [r for _, _, _, r in trajectories]\n",
        "    mean_reward = np.mean(rewards)\n",
        "    std_reward = np.std(rewards) + 1e-8\n",
        "    advantages = [(r - mean_reward) / std_reward for r in rewards]\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        loss = 0\n",
        "        for (states, old_log_probs, actions, _), advantage in zip(trajectories, advantages):\n",
        "            trajectory_loss = 0\n",
        "            for t in range(len(states)):\n",
        "                state_tensor = torch.FloatTensor(states[t])\n",
        "                new_logits = net(state_tensor)\n",
        "                new_probs = torch.softmax(new_logits, dim=-1)\n",
        "                new_log_prob = torch.log(new_probs[actions[t]])\n",
        "                ratio = torch.exp(new_log_prob - old_log_probs[t])\n",
        "                clipped_ratio = torch.clamp(ratio, 1 - eps, 1 + eps)\n",
        "                trajectory_loss += -torch.min(ratio * advantage, clipped_ratio * advantage)\n",
        "            loss += trajectory_loss / len(states)\n",
        "        loss /= len(trajectories)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# メインのトレーニング関数\n",
        "def train_algorithm(algorithm, env_name='CartPole-v0', num_episodes=5000, lr=0.001):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # アルゴリズムに応じたネットワークとオプティマイザーの初期化\n",
        "    if algorithm in ['reinforce', 'grpo']:\n",
        "        agent = PolicyNet(state_dim, action_dim)\n",
        "    else:  # actor-critic or ppo\n",
        "        agent = ActorCriticNet(state_dim, action_dim)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
        "\n",
        "    episode_rewards = deque(maxlen=100)\n",
        "    trajectories = []  # GRPO用\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        states, actions, log_probs, total_reward = run_episode(env, agent, algorithm)\n",
        "\n",
        "        if algorithm == 'reinforce':\n",
        "            reinforce_update(agent, optimizer, states, actions, log_probs, total_reward)\n",
        "        elif algorithm == 'actor-critic':\n",
        "            actor_critic_update(agent, optimizer, states, actions, total_reward)\n",
        "        elif algorithm == 'ppo':\n",
        "            ppo_update(agent, optimizer, states, actions, total_reward)\n",
        "        elif algorithm == 'grpo':\n",
        "            trajectories.append((states, log_probs, actions, total_reward))\n",
        "            if len(trajectories) >= 5:  # 5つのトラジェクトリを収集したら更新\n",
        "                grpo_update(agent, optimizer, trajectories)\n",
        "                trajectories = []\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        avg_reward = np.mean(episode_rewards)\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f'{algorithm.upper()} Episode {episode}, Avg Reward: {avg_reward:.2f}')\n",
        "\n",
        "        if avg_reward > 195:\n",
        "            print(f'{algorithm.upper()} Solved in {episode} episodes!')\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    return agent, episode_rewards\n",
        "\n",
        "# 4つのアルゴリズムを比較実行\n",
        "if __name__ == '__main__':\n",
        "    algorithms = ['reinforce', 'actor-critic', 'ppo', 'grpo']\n",
        "    results = {}\n",
        "\n",
        "    for algo in algorithms:\n",
        "        print(f\"\\nTraining {algo.upper()}\")\n",
        "        agent, rewards = train_algorithm(algo)\n",
        "        results[algo] = rewards\n",
        "\n",
        "    # 結果の簡単な比較（オプション）\n",
        "    for algo, rewards in results.items():\n",
        "        print(f\"{algo.upper()} Final Avg Reward: {np.mean(list(rewards)[-100:]):.2f}\")"
      ]
    }
  ]
}