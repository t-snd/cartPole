{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhGPITAkJx+t7EK2c3nsKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t-snd/cartPole/blob/main/testCartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsNoKwifTbMe",
        "outputId": "e28a4249-40fb-4dc6-ff29-bf1d6a9f1cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:519: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 0 avg_reward 16.0\n",
            "episode 100 avg_reward 21.37\n",
            "episode 200 avg_reward 24.7\n",
            "episode 300 avg_reward 32.65\n",
            "episode 400 avg_reward 30.68\n",
            "episode 500 avg_reward 45.83\n",
            "episode 600 avg_reward 55.12\n",
            "episode 700 avg_reward 63.6\n",
            "episode 800 avg_reward 83.98\n",
            "episode 900 avg_reward 76.84\n",
            "episode 1000 avg_reward 109.59\n",
            "episode 1100 avg_reward 109.31\n",
            "episode 1200 avg_reward 117.84\n",
            "episode 1300 avg_reward 127.42\n",
            "episode 1400 avg_reward 145.33\n",
            "episode 1500 avg_reward 166.29\n",
            "episode 1600 avg_reward 168.66\n",
            "episode 1700 avg_reward 180.45\n",
            "episode 1800 avg_reward 183.08\n",
            "episode 1900 avg_reward 181.85\n",
            "episode 2000 avg_reward 182.38\n",
            "episode 2100 avg_reward 186.64\n",
            "episode 2200 avg_reward 181.18\n",
            "episode 2300 avg_reward 187.71\n",
            "solved at episode 2383\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "import torch\n",
        "\n",
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(4, 64)\n",
        "        self.fc2 = torch.nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = PolicyNet()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "gamma = 0.99\n",
        "\n",
        "episode_reward_window = []\n",
        "\n",
        "for i_episode in range(5000):\n",
        "    observation = env.reset()[0]\n",
        "    log_probs = []\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(200):\n",
        "        # get prob distribution over actions\n",
        "        logits = net(torch.from_numpy(observation).float())\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        # sample an action\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        # take the action\n",
        "        observation, reward, done, _, info = env.step(action)\n",
        "        # save prob of chosen action and reward\n",
        "        log_prob = torch.log(probs[action])\n",
        "        log_probs.append(log_prob)\n",
        "\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    normalized_reward = episode_reward / 200.0\n",
        "    # calculate policy gradient loss\n",
        "    policy_loss = []\n",
        "    for log_prob in log_probs:\n",
        "        policy_loss.append(-log_prob * normalized_reward)\n",
        "\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "\n",
        "    # update the weights\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    episode_reward_window.append(episode_reward)\n",
        "    if len(episode_reward_window) > 100:\n",
        "        episode_reward_window.pop(0)\n",
        "    avg_reward = sum(episode_reward_window) / len(episode_reward_window)\n",
        "\n",
        "    if avg_reward > 195:\n",
        "        print('solved at episode', i_episode)\n",
        "        break\n",
        "\n",
        "    if i_episode % 100 == 0:\n",
        "        print('episode', i_episode, 'avg_reward', avg_reward)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor-Critic"
      ],
      "metadata": {
        "id": "QcXr7eErU67p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "z_YnePVAVAWq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorCriticNet, self).__init__()\n",
        "\n",
        "        # Actor network (policy)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "\n",
        "        # Critic network (value)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Returns action logits and state value\n",
        "        return self.actor(state), self.critic(state)"
      ],
      "metadata": {
        "id": "i2ZtfhNYVFtb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic:\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001):\n",
        "        self.model = ActorCriticNet(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.gamma = 0.99\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        action_logits, _ = self.model(state)\n",
        "        action_probs = torch.softmax(action_logits, dim=-1)\n",
        "        action = torch.multinomial(action_probs, 1).item()\n",
        "        return action\n",
        "\n",
        "    def train(self, states, actions, total_reward, terminal_state):\n",
        "        # Convert lists to tensors\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "\n",
        "        # Compute returns (works because of sparse reward)\n",
        "        returns = torch.zeros_like(states[:, 0], dtype=torch.float)\n",
        "        returns[-1] = total_reward\n",
        "\n",
        "        # Compute loss for each step\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "\n",
        "        for t in reversed(range(len(states) - 1)):\n",
        "            # Actor-Critic update\n",
        "            action_logits, value = self.model(states[t].unsqueeze(0))\n",
        "            _, next_value = self.model(states[t+1].unsqueeze(0))\n",
        "\n",
        "            # Compute advantage (using sparse total reward)\n",
        "            advantage = total_reward - value.detach()\n",
        "\n",
        "            # Actor loss (policy gradient)\n",
        "            action_probs = torch.softmax(action_logits, dim=-1)\n",
        "            log_prob = torch.log(action_probs[0, actions[t]])\n",
        "            actor_loss = -log_prob * advantage\n",
        "            actor_losses.append(actor_loss)\n",
        "\n",
        "            # Critic loss\n",
        "            critic_loss = advantage.pow(2)\n",
        "            critic_losses.append(critic_loss)\n",
        "\n",
        "        # Combine and backpropagate\n",
        "        total_loss = torch.stack(actor_losses).sum() + 0.5 * torch.stack(critic_losses).sum()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return total_loss.item()"
      ],
      "metadata": {
        "id": "WwSa1eIEVLdo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sparse_reward_actor_critic(env_name='CartPole-v1', num_episodes=5000):\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Get state and action dimensions\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Initialize Actor-Critic agent\n",
        "    agent = ActorCritic(state_dim, action_dim)\n",
        "\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        # For sparse reward tracking\n",
        "        states = []\n",
        "        actions = []\n",
        "\n",
        "        while not done:\n",
        "            # Select action\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "            # Store state and action for later training\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Sparse reward: total steps as reward\n",
        "        total_reward = len(states)\n",
        "\n",
        "        # Train the agent with all collected states and actions\n",
        "        agent.train(states, actions, total_reward, state)\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        # Compute moving average\n",
        "        if len(episode_rewards) > 100:\n",
        "            episode_rewards.pop(0)\n",
        "        avg_reward = np.mean(episode_rewards)\n",
        "\n",
        "        # Print progress\n",
        "        if episode % 100 == 0:\n",
        "            print(f'Episode {episode}, Average Reward: {avg_reward:.2f}')\n",
        "\n",
        "        # Check if environment is solved\n",
        "        if avg_reward > 195:\n",
        "            print(f'Solved in {episode} episodes!')\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    return agent"
      ],
      "metadata": {
        "id": "Nl_ctLgiVSrh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = train_sparse_reward_actor_critic()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85WU-ROuU6oc",
        "outputId": "93edd86c-cad2-4d81-fd09-5de04fa4c2fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-087091663f6f>:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  states = torch.FloatTensor(states)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Average Reward: 24.00\n",
            "Episode 100, Average Reward: 26.92\n",
            "Episode 200, Average Reward: 35.33\n",
            "Episode 300, Average Reward: 39.68\n",
            "Episode 400, Average Reward: 45.00\n",
            "Episode 500, Average Reward: 44.31\n",
            "Episode 600, Average Reward: 52.33\n",
            "Episode 700, Average Reward: 73.87\n",
            "Episode 800, Average Reward: 65.07\n",
            "Episode 900, Average Reward: 101.50\n",
            "Episode 1000, Average Reward: 105.41\n",
            "Episode 1100, Average Reward: 141.84\n",
            "Episode 1200, Average Reward: 166.82\n",
            "Episode 1300, Average Reward: 162.92\n",
            "Solved in 1340 episodes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO"
      ],
      "metadata": {
        "id": "5ctIQDPZjAxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPONet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PPONet, self).__init__()\n",
        "\n",
        "        # Actor network (policy)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Critic network (value)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Returns action probabilities and state value\n",
        "        return self.actor(state), self.critic(state)"
      ],
      "metadata": {
        "id": "erL5KF7YjDIT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001,\n",
        "                 clip_ratio=0.2, value_loss_coef=0.5, entropy_coef=0.01):\n",
        "        self.model = PPONet(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # PPO hyperparameters\n",
        "        self.clip_ratio = clip_ratio\n",
        "        self.value_loss_coef = value_loss_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.gamma = 0.99\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        action_probs, _ = self.model(state)\n",
        "        action = torch.multinomial(action_probs, 1).item()\n",
        "        return action\n",
        "\n",
        "    def train(self, states, actions, total_reward):\n",
        "        # Convert lists to tensors\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "\n",
        "        # Compute old action probabilities\n",
        "        old_action_probs, old_values = self.model(states)\n",
        "        old_action_probs = old_action_probs.detach()\n",
        "        old_values = old_values.detach()\n",
        "\n",
        "        # Get current action probabilities and values\n",
        "        curr_action_probs, curr_values = self.model(states)\n",
        "\n",
        "        # Compute advantages\n",
        "        advantages = total_reward - old_values.squeeze()\n",
        "\n",
        "        # PPO policy loss\n",
        "        ratios = curr_action_probs.gather(1, actions.unsqueeze(1)).squeeze() / \\\n",
        "                 old_action_probs.gather(1, actions.unsqueeze(1)).squeeze()\n",
        "        surr1 = ratios * advantages\n",
        "        surr2 = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
        "        policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "        # Value loss\n",
        "        value_loss = (curr_values.squeeze() - total_reward).pow(2).mean()\n",
        "\n",
        "        # Entropy loss for exploration\n",
        "        entropy_loss = -(curr_action_probs * torch.log(curr_action_probs + 1e-10)).sum(dim=1).mean()\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = (policy_loss +\n",
        "                      self.value_loss_coef * value_loss -\n",
        "                      self.entropy_coef * entropy_loss)\n",
        "\n",
        "        # Backpropagate\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return total_loss.item()\n"
      ],
      "metadata": {
        "id": "zBrHmbasjHgO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sparse_reward_ppo(env_name='CartPole-v1', num_episodes=5000):\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Get state and action dimensions\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Initialize PPO agent\n",
        "    agent = PPO(state_dim, action_dim)\n",
        "\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        # For sparse reward tracking\n",
        "        states = []\n",
        "        actions = []\n",
        "\n",
        "        while not done:\n",
        "            # Select action\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "            # Store state and action for later training\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Sparse reward: total steps as reward\n",
        "        total_reward = len(states)\n",
        "\n",
        "        # Train the agent with all collected states and actions\n",
        "        agent.train(states, actions, total_reward)\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        # Compute moving average\n",
        "        if len(episode_rewards) > 100:\n",
        "            episode_rewards.pop(0)\n",
        "        avg_reward = np.mean(episode_rewards)\n",
        "\n",
        "        # Print progress\n",
        "        if episode % 100 == 0:\n",
        "            print(f'Episode {episode}, Average Reward: {avg_reward:.2f}')\n",
        "\n",
        "        # Check if environment is solved\n",
        "        if avg_reward > 195:\n",
        "            print(f'Solved in {episode} episodes!')\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    return agent\n"
      ],
      "metadata": {
        "id": "-1pCDkH3jKrX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = train_sparse_reward_ppo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPY9dzQjjNFz",
        "outputId": "36e9d19c-ea80-4aed-c0d7-26bb2bb2c04c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Average Reward: 19.00\n",
            "Episode 100, Average Reward: 20.21\n",
            "Episode 200, Average Reward: 19.84\n",
            "Episode 300, Average Reward: 17.67\n",
            "Episode 400, Average Reward: 24.64\n",
            "Episode 500, Average Reward: 26.97\n",
            "Episode 600, Average Reward: 31.63\n",
            "Episode 700, Average Reward: 36.26\n",
            "Episode 800, Average Reward: 46.76\n",
            "Episode 900, Average Reward: 63.60\n",
            "Episode 1000, Average Reward: 88.56\n",
            "Episode 1100, Average Reward: 154.16\n",
            "Solved in 1181 episodes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRPO"
      ],
      "metadata": {
        "id": "MpWiZhgckB2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(4, 64)\n",
        "        self.fc2 = torch.nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def collect_trajectory(env, net):\n",
        "    observation = env.reset()[0]\n",
        "    log_probs = []\n",
        "    observations = []\n",
        "    chosen_actions = []\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(200):\n",
        "        observations.append(observation)\n",
        "        logits = net(torch.from_numpy(observation).float())\n",
        "        probs = torch.nn.functional.softmax(logits, dim=0)\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        observation, reward, done, _, info = env.step(action)\n",
        "        log_prob = torch.log(probs[action])\n",
        "        log_probs.append(log_prob.item())\n",
        "        chosen_actions.append(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    normalized_reward = episode_reward / 200.0 # because max reward possible in this env is 200\n",
        "    return observations, log_probs, chosen_actions, normalized_reward\n",
        "\n",
        "\n",
        "def grpo_update(trajectories, net, optimizer, n_iterations=20):\n",
        "    rewards = [r for o, l, a, r in trajectories]\n",
        "    mean_reward = sum(rewards) / len(rewards)\n",
        "    std_reward = np.std(rewards) + 1e-8\n",
        "    advantages = [(r - mean_reward) / std_reward for r in rewards]\n",
        "\n",
        "    for i_iter in range(n_iterations):\n",
        "        loss = 0\n",
        "        # iterating over each trajectory in the group\n",
        "        for traj, advantage in zip(trajectories, advantages):\n",
        "            (observations, log_probs, chosen_actions, _) = traj\n",
        "            trajectory_loss = 0\n",
        "            # iterating over each time step in the trajectory\n",
        "            for t in range(len(observations)):\n",
        "                new_policy_probs = torch.nn.functional.softmax(net(torch.from_numpy(observations[t]).float()), dim=0)\n",
        "                new_log_probs = torch.log(new_policy_probs)[chosen_actions[t]]\n",
        "\n",
        "                ratio = torch.exp(new_log_probs - log_probs[t])\n",
        "                clipped_ratio = torch.clamp(ratio, min=1 - eps, max=1 + eps)\n",
        "                trajectory_loss += -clipped_ratio * advantage\n",
        "            trajectory_loss /= len(observations)\n",
        "            loss += trajectory_loss\n",
        "        loss /= len(trajectories)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "net = PolicyNet()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "episode_reward_window = deque(maxlen=100)\n",
        "\n",
        "# GRPO specific parameters\n",
        "trajectories_per_update = 5  # group size\n",
        "# epsilon for clipping\n",
        "eps = 0.2\n",
        "\n",
        "# training loop\n",
        "for i_episode in range(5000):\n",
        "    trajectories = []\n",
        "    episode_rewards = []\n",
        "\n",
        "    for _ in range(trajectories_per_update):\n",
        "        observations, log_probs, chosen_actions, normalized_reward = collect_trajectory(env, net)\n",
        "        trajectories.append((observations, log_probs, chosen_actions, normalized_reward))\n",
        "        episode_rewards.append(normalized_reward * 200)  # unnormalize for tracking\n",
        "\n",
        "    # update policy using grpo on the collected trajectories\n",
        "    grpo_update(trajectories, net, optimizer)\n",
        "\n",
        "    episode_reward_window.extend(episode_rewards)\n",
        "    avg_reward = sum(episode_reward_window) / len(episode_reward_window)\n",
        "\n",
        "    if avg_reward > 195:\n",
        "        print('solved at episode', i_episode)\n",
        "        break\n",
        "\n",
        "    if i_episode % 10 == 0:\n",
        "        print(f'episode {i_episode}, avg reward: {avg_reward:.2f}')\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isvUr2L9kDth",
        "outputId": "bb450800-067c-4bd8-b144-f3c288ec856d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:519: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 0, avg reward: 19.80\n",
            "episode 10, avg reward: 34.44\n",
            "episode 20, avg reward: 51.11\n",
            "episode 30, avg reward: 79.44\n",
            "episode 40, avg reward: 133.47\n",
            "episode 50, avg reward: 179.92\n",
            "episode 60, avg reward: 192.34\n",
            "solved at episode 67\n"
          ]
        }
      ]
    }
  ]
}